#!/usr/bin/env python

import imp
import logging
import os
import string
import zipfile

import portia2code.parser
import portia2code.processors
import portia2code.spiders

from datetime import datetime
from inspect import getsource
from itertools import chain
from os.path import join

from six import StringIO

from autoflake import fix_code
from autopep8 import fix_lines
from scrapy.commands.startproject import TEMPLATES_PATH
from scrapy.utils.template import string_camelcase
from slybot.spider import IblSpider
from slybot.starturls import generator
from portia2code.samples import ItemBuilder
from portia2code.templates import (
    ITEM_CLASS, ITEM_FIELD, ITEMS_IMPORTS, RULES, SPIDER_CLASS, SPIDER_FILE
)
from portia2code.utils import (PROCESSOR_TYPES, _validate_identifier, _clean,
                               class_name, item_field_name)
log = logging.getLogger(__name__)


class Options(object):
    """Settings for autopep8."""

    version = '1.0.0'
    verbose = None
    diff = False
    in_place = False
    global_config = False
    ignore_local_config = False
    recursive = False
    jobs = 1
    pep8_passes = -1
    aggressive = 3
    experimental = False
    exclude = ''
    list_fixes = False
    ignore = ''
    select = ''
    max_line_length = 79
    line_range = None
    indent_size = 4
    files = []


def write_to_archive(archive, project_name, files):
    """Write files to the project_name folder of the archive."""
    tstamp = datetime.now().timetuple()[:6]
    for filepath, contents in files:
        if not contents:
            continue
        if filepath is None or contents in (None, 'null'):
            log.debug('Skipping file "%s" with contents "%r"' % (filepath,
                                                                 contents))
            continue
        filepath = join(project_name, filepath)
        fileinfo = zipfile.ZipInfo(filepath, tstamp)
        fileinfo.external_attr = 0o666 << 16
        archive.writestr(fileinfo, contents, zipfile.ZIP_DEFLATED)


def find_files(project_name):
    sep = os.sep
    read_files = {}
    for base, dirs, files in os.walk(join(TEMPLATES_PATH)):
        basepath = base[len(TEMPLATES_PATH):]
        if basepath.lstrip('/\\').startswith('module'):
            basepath = join(project_name, sep.join(basepath.split(sep)[2:]))
        for filename in files:
            if filename.endswith(('.pyc', '.pyo')):
                continue
            path = join(base, filename)
            with open(path) as f:
                out_path = join(basepath, filename)
                read_files[out_path] = f.read()
    return read_files


def start_scrapy_project(project_name):
    files = find_files(project_name)
    out_files = {}
    for path, contents in files.items():
        contents = string.Template(contents).substitute(
            project_name=project_name,
            ProjectName=string_camelcase(project_name)
        )
        if path.endswith('.tmpl'):
            path = path[:-len('.tmpl')]
        out_files[path] = contents
    return out_files


def create_schemas_classes(items):
    """Create schemas and fields from definitions."""
    item_classes, item_names = [], {}
    for item_id, item in items.items():
        item_name = class_name(item.get('name', item_id))
        if not _validate_identifier(item_name):
            log.warning(
                'Skipping item with id "%s", name "%s" is not a valid '
                'identifier' % (item_id, item_name))
            continue
        item_fields = ''.join(create_fields(item['fields']))
        if not item_fields:
            item_fields = 'pass\n'.rjust(9)
        item_classes.append(
            ITEM_CLASS(name=item_name, fields=item_fields))
        item_names[item_id] = item_name
    return item_classes, item_names


def create_fields(item_fields):
    """"Create fields from definitions."""
    fields = []
    for field_id, field in item_fields.items():
        name = item_field_name(field.get('name', field_id))
        if not _validate_identifier(name):
            log.warning(
                'Skipping field with id "%s", name "%s" is not a valid '
                'identifier' % (field_id, name))
            continue
        field_type = field.get('type', 'text')
        input_processor = repr(PROCESSOR_TYPES.get(field_type, 'lambda x: x'))
        output_processor = 'Join()'
        fields.append(ITEM_FIELD(name=name, input=input_processor,
                                 output=output_processor))
    return fields


def create_library_files():
    """Write utilities needed to run spiders."""
    return [
        ('utils/__init__.py', ''),
        ('utils/parser.py', getsource(portia2code.parser)),
        ('utils/processors.py', getsource(portia2code.processors)),
        ('utils/spiders.py', getsource(portia2code.spiders)),
        ('utils/starturls.py', getsource(generator))
    ]


def create_schemas(items):
    """Create and write schemas from definitions."""
    schema_classes, schema_names = create_schemas_classes(items)
    items_py = '\n'.join(chain([ITEMS_IMPORTS], schema_classes)).strip()
    items_py = fix_lines(fix_code(items_py.decode('utf-8')).splitlines(),
                         Options)
    return items_py, schema_names


def create_spider(name, spider, spec, samples, schemas, extractors, items):
    cls_name = class_name(name)
    urls_type = getattr(spider, 'start_urls_type', 'start_urls')
    start_urls = []
    if urls_type == 'start_urls':
        urls = getattr(spider, 'start_urls', []) or spec.get('start_urls', [])
        if urls:
            start_urls = repr(urls)
    elif urls_type == 'generated_urls':
        urls_spec = (getattr(spider, 'generated_urls', []) or
                     spec.get('generated_urls', []))
        if urls_spec:
            start_urls = 'generator(%r)' % urls_spec

    allowed = spider.allowed_domains
    crawling_options = spec.get('links_to_follow')
    allow, deny = '', ''
    if crawling_options == 'patterns':
        if spec.get('follow_patterns'):
            allow = ', '.join((repr(s) for s in spec['follow_patterns']))
        if spec.get('exclude_patterns'):
            deny = ','.join((repr(s) for s in spec['exclude_patterns']))
    elif crawling_options == 'none':
        deny = "'.*'"
    else:
        allow = "'.*'"
    # TODO: Add support for auto
    rules = RULES(allow=allow, deny=deny)
    item_imports = ItemBuilder(
        schemas, extractors, items, items['_PortiaItem']).extract(
        spider.plugins[0].extractors
    )
    return SPIDER_CLASS(
        class_name=cls_name, name=name, allowed_domains=repr(allowed),
        start_urls=start_urls, rules=rules, items=item_imports
    )


def create_spiders(spiders, schemas, extractors, items):
    item_classes = ''
    if items:
        item_classes = '\nfrom ..items import {}'.format(
            ', '.join((v().__class__.__name__ for v in items.values()))
        )
    spider_data = []
    for name, (spider, spec) in spiders.items():
        log.info('Creating spider "%s"' % spider.name)
        spider = create_spider(name, spider, spec, spec['templates'],
                               schemas, extractors, items)
        cleaned_name = _clean(name)
        filename = 'spiders/{}.py'.format(cleaned_name)
        data = '\n'.join((SPIDER_FILE(item_classes=item_classes),
                          spider.strip()))
        code = fix_lines(fix_code(data.decode('utf-8')).splitlines(), Options)
        spider_data.append((filename, code))
    return spider_data


def port_project(dir_name, schemas, spiders, extractors):
    # Create project layout and default files
    zbuff = StringIO()
    archive = zipfile.ZipFile(zbuff, "w", zipfile.ZIP_DEFLATED)
    write_to_archive(archive, '', start_scrapy_project(dir_name).items())
    items_py, schema_names = create_schemas(schemas)
    write_to_archive(archive, dir_name, [('items.py', items_py)])
    write_to_archive(archive, dir_name, create_library_files())

    # XXX: Hack to load items.py file
    items_no_relative = items_py.replace(
        'from .utils.processors import', 'from portia2code.processors import'
    )
    mod = imp.new_module('%s.%s' % (dir_name, 'items'))
    exec(items_no_relative, mod.__dict__)
    items = vars(mod)

    # Load schema objects from module
    schema_names = {}
    for _id, name in schema_names.items():
        schema_names[_id] = items['%sItem' % name]
    schema_names['_PortiaItem'] = items['PortiaItem']

    spider_data = create_spiders(spiders, schemas, extractors, schema_names)
    write_to_archive(archive, dir_name, spider_data)
    archive.close()
    zbuff.seek(0)
    return zbuff


if __name__ == '__main__':
    logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s',
                        datefmt='%Y-%m-%d %H:%M:%S',
                        level=logging.INFO)
    import json
    import sys

    from scrapy.settings import Settings
    from slybot.utils import _build_sample
    project_dir = sys.argv[1]
    try:
        out_dir = os.path.abspath(sys.argv[2])
        dir_name = os.path.split(out_dir)[-1]
    except IndexError:
        out_dir = os.path.abspath('./')
        dir_name = os.path.split(os.path.abspath(project_dir))[-1]
    if dir_name.endswith('.zip'):
        out_dir = os.path.dirname(dir_name)
        dir_name = dir_name[:-len('.zip')]
    if not _validate_identifier(dir_name):
        raise ValueError('Output project name "%s" is not a valid name. Valid '
                         'names should only contain number letters and '
                         'underscores and may not start with a number' %
                         dir_name)

    # Load items and extractors from project
    with open(join(project_dir, 'items.json')) as f:
        schemas = json.load(f)
    with open(join(project_dir, 'extractors.json')) as f:
        extractors = json.load(f)

    # Load spiders and templates
    spiders = {}
    spider_dir = join(project_dir, 'spiders')
    spiders_list = [s for s in os.listdir(spider_dir) if s.endswith('.json')]
    for spider_file in spiders_list:
        spider_name = spider_file[:-5]
        with open(join(spider_dir, spider_file))as f:
            spider = json.load(f)
        if not spider:
            log.warning(
                'Skipping "{}" spider as there is no data'.format(spider_name)
            )
            continue
        if 'template_names' in spider:
            samples = spider.get('template_names', [])
            spider['templates'] = []
            for sample_name in samples:
                sample_file = '%s.json' % sample_name
                sample_path = join(spider_dir, spider_name, sample_file)
                with open(sample_path) as f:
                    sample = json.load(f)
                    _build_sample(sample)
                    spider['templates'].append(sample)
        else:
            for sample in spider.get('templates', []):
                _build_sample(sample)
        spiders[spider_name] = (IblSpider(spider_name, spider, schemas,
                                          extractors, Settings()),
                                spider)

    # Write contents to file
    out_path = os.path.join(out_dir, '%s.zip' % dir_name)
    log.info('Writing project to "%s"' % out_path)
    with open(out_path, 'wb') as f:
        f.write(port_project(dir_name, schemas, spiders, extractors).read())
    log.info('Finished.')
