Installation
============

To install `portia_porter`:

    git clone https://github.com/scrapinghub/portia2code.git
    cd portia2code
    python setup.py install    


Port your project
=================

Once You have installed the tool you can port your project using the command:

    portia_porter PROJECT_DIR OUT_DIR


If you use the hosted version of portia on scrapinghub you can port it by
running:

    export SHUB_APIKEY="APIKEY"
    export PROJECT_ID="PROJECT_ID"
    export PROJECT_DIR="PROJECT_DIR"
    export PROJECT_NAME="PROJECT_NAME"
    curl --user $SHUB_APIKEY: https://portia.scrapinghub.com/api/projects/$PROJECT_ID/download \
            > /tmp/portia_project.zip &&
        unzip /tmp/portia_project.zip -d /tmp/portia_project &&
        portia_porter /tmp/portia_project $PROJECT_DIR/$PROJECT_NAME.zip &&
        rm -r /tmp/portia_project &&
        rm /tmp/portia_project.zip

How it works
============

Spiders built with Portia consist of JSON definitions of which urls a spider
should crawl and how it should extract data from the data at those urls.
When running these spiders the JSON definitions are compiled into a custom
Scrapy spider with trained samples used for extraction. The trained samples
are used with the `scrapely` library to extract data from similar pages. To
build one of these trained samples an annotated page must be passed to scrapely
so it knows what data to extract. In older versions of Portia these annotations
were found in the page by adding an attribute to each element called
`data-tagid=N` where `N` is a unique incrementing integer for each open or self
closing tag on the page. Newer versions of Portia have changed this and now use
unique selectors for each annotated element on the page. Using these unique
selectors we are able to build an extraction tree that can use item loaders on
a page to extract the annotated data.


Missing Features
================

Some features from Portia are still not available though this porting mechanism
but will hopefully be added in the future:

* Load pages using Splash depending on crawl rules
* Follow links automatically
* Text data extractors (annotations generated by highlighting text)


Future Improvements
===================

* The format used for repeated annotations is inflexible and doesn't allow for
  additional items to be extracted if there are more on the crawled page.
* Only Portia 2.0 spiders are supported for now but we will be adding support
  for Portia 1.0 spiders.